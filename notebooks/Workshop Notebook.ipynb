{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "workshop_header"
      },
      "source": [
        "# üå°Ô∏è Climate Model Validation Workshop\n",
        "\n",
        "## Complete End-to-End Analysis\n",
        "**Study Area:** AMMAN ZARQA Basin, Jordan  \n",
        "**Models:** 6 RICCAR Climate Models (SSP4.5)  \n",
        "**Stations:** AL0019, AL0035, AL0059  \n",
        "**Period:** 1990-2014\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Workshop Overview\n",
        "\n",
        "This notebook contains the complete climate model validation workflow:\n",
        "\n",
        "### **Part 1: Station Data Extraction** üìç\n",
        "- Download climate model NetCDF files\n",
        "- Extract temperature data at station locations\n",
        "- Assess spatial resolution and data quality\n",
        "\n",
        "### **Part 2: Monthly Climatology** üìä\n",
        "- Convert daily data to monthly climatology\n",
        "- Calculate seasonal temperature patterns\n",
        "- Visualize model seasonal cycles\n",
        "\n",
        "### **Part 3: Station Data Processing** üå°Ô∏è\n",
        "- Load observed temperature data\n",
        "- Process station climatology\n",
        "- Prepare observational reference\n",
        "\n",
        "### **Part 4: Model Validation** üéì\n",
        "- Calculate 5 validation metrics (RMSE, R, NSE, PBIAS, MAE)\n",
        "- Compare models against observations\n",
        "- Rank model performance\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Learning Objectives\n",
        "By the end of this workshop, you will:\n",
        "- ‚úÖ Understand climate model validation workflow\n",
        "- ‚úÖ Extract and process climate data using Python\n",
        "- ‚úÖ Calculate validation metrics and interpret results\n",
        "- ‚úÖ Create publication-quality visualizations\n",
        "- ‚úÖ Make informed decisions about model selection\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è±Ô∏è Estimated Time: 45-60 minutes\n",
        "\n",
        "**Ready to start? Let's validate some climate models!** üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "# ‚öôÔ∏è Workshop Setup: Install Packages and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install xarray netcdf4 requests tqdm seaborn -q\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "print(f\"üìç Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [],
      "source": [
        "def download_workshop_data():\n",
        "    \"\"\"Download all required files from GitHub repository\"\"\"\n",
        "    \n",
        "    print(\"üéØ DOWNLOADING WORKSHOP DATA\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # GitHub repository base URL\n",
        "    base_url = \"https://raw.githubusercontent.com/MoawiahHussien/climate-model-validation-workshop/main/\"\n",
        "    \n",
        "    # Create directories\n",
        "    os.makedirs(\"input_data/models_netcdf\", exist_ok=True)\n",
        "    os.makedirs(\"input_data/stations_data\", exist_ok=True)\n",
        "    os.makedirs(\"workshop_output\", exist_ok=True)\n",
        "    \n",
        "    # NetCDF files to download\n",
        "    nc_files = [\n",
        "        \"arcgis_merged_Tmax_CMCC-CM2-SR5.nc\",\n",
        "        \"arcgis_merged_Tmax_CNRM-ESM2-1.nc\", \n",
        "        \"arcgis_merged_Tmax_EC-Earth3-Veg.nc\",\n",
        "        \"arcgis_merged_Tmax_IPSL-CM6A-LR.nc\",\n",
        "        \"arcgis_merged_Tmax_MPI-ESM1-2-LR.nc\",\n",
        "        \"arcgis_merged_Tmax_NorESM2-MM.nc\"\n",
        "    ]\n",
        "    \n",
        "    # Station files to download\n",
        "    station_files = [\"AL0019.xlsx\", \"AL0035.xlsx\", \"AL0059.xlsx\"]\n",
        "    \n",
        "    # Download NetCDF files\n",
        "    print(\"üì• Downloading climate model files...\")\n",
        "    for nc_file in nc_files:\n",
        "        file_url = base_url + \"Input%20Files/Models.Nc.ArcGIS.Compatible/\" + nc_file\n",
        "        local_path = f\"input_data/models_netcdf/{nc_file}\"\n",
        "        \n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"‚úÖ Using cached: {nc_file}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(f\"üì• Downloading: {nc_file}\")\n",
        "            urllib.request.urlretrieve(file_url, local_path)\n",
        "            file_size = os.path.getsize(local_path) / (1024*1024)\n",
        "            print(f\"   ‚úÖ Complete: {file_size:.1f} MB\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Failed: {e}\")\n",
        "    \n",
        "    # Download station files\n",
        "    print(\"\\nüì• Downloading station files...\")\n",
        "    for station_file in station_files:\n",
        "        file_url = base_url + \"Input%20Files/Stations.Daily/\" + station_file\n",
        "        local_path = f\"input_data/stations_data/{station_file}\"\n",
        "        \n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"‚úÖ Using cached: {station_file}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(f\"üì• Downloading: {station_file}\")\n",
        "            urllib.request.urlretrieve(file_url, local_path)\n",
        "            file_size = os.path.getsize(local_path) / (1024*1024)\n",
        "            print(f\"   ‚úÖ Complete: {file_size:.1f} MB\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Failed: {e}\")\n",
        "    \n",
        "    print(f\"\\nüéâ Data download complete!\")\n",
        "\n",
        "# Download the data\n",
        "download_workshop_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_constants"
      },
      "outputs": [],
      "source": [
        "# Define workshop configuration\n",
        "stations = {\n",
        "    'AL0019': {'lat': 31.95, 'lon': 35.93, 'name': 'Amman Airport'},\n",
        "    'AL0035': {'lat': 32.01, 'lon': 35.85, 'name': 'Zarqa Station'}, \n",
        "    'AL0059': {'lat': 31.97, 'lon': 36.12, 'name': 'Russeifa Station'}\n",
        "}\n",
        "\n",
        "model_files = {\n",
        "    'CMCC': 'arcgis_merged_Tmax_CMCC-CM2-SR5.nc',\n",
        "    'CNRM': 'arcgis_merged_Tmax_CNRM-ESM2-1.nc',\n",
        "    'EC-Earth3': 'arcgis_merged_Tmax_EC-Earth3-Veg.nc',\n",
        "    'IPSL': 'arcgis_merged_Tmax_IPSL-CM6A-LR.nc',\n",
        "    'MPI': 'arcgis_merged_Tmax_MPI-ESM1-2-LR.nc',\n",
        "    'NorESM2': 'arcgis_merged_Tmax_NorESM2-MM.nc'\n",
        "}\n",
        "\n",
        "model_colors = {\n",
        "    'CMCC': '#1f77b4', 'CNRM': '#ff7f0e', 'EC-Earth3': '#2ca02c',\n",
        "    'IPSL': '#d62728', 'MPI': '#9467bd', 'NorESM2': '#8c564b'\n",
        "}\n",
        "\n",
        "print(f\"üéØ WORKSHOP CONFIGURATION\")\n",
        "print(f\"üìç Target Stations: {len(stations)}\")\n",
        "print(f\"üå°Ô∏è Climate Models: {len(model_files)}\")\n",
        "\n",
        "for station_id, info in stations.items():\n",
        "    print(f\"  {station_id}: {info['name']} ({info['lat']:.2f}¬∞N, {info['lon']:.2f}¬∞E)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_header"
      },
      "source": [
        "# üìç Part 1: Station Data Extraction from Climate Models\n",
        "\n",
        "Extract temperature data at our 3 weather station locations from 6 climate models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_station_data"
      },
      "outputs": [],
      "source": [
        "def extract_station_data():\n",
        "    \"\"\"Extract temperature data at station locations from all models\"\"\"\n",
        "    \n",
        "    print(f\"üìç EXTRACTING DATA AT STATION LOCATIONS\")\n",
        "    print(\"-\" * 45)\n",
        "    \n",
        "    # Storage for results\n",
        "    model_station_data = {}\n",
        "    extraction_log = []\n",
        "    \n",
        "    # Process each model\n",
        "    for model_name, filename in model_files.items():\n",
        "        print(f\"\\nüîÑ Processing: {model_name}\")\n",
        "        \n",
        "        file_path = f\"input_data/models_netcdf/{filename}\"\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"    ‚ùå File not found: {filename}\")\n",
        "            continue\n",
        "        \n",
        "        # Open NetCDF file\n",
        "        ds = xr.open_dataset(file_path)\n",
        "        model_station_data[model_name] = {}\n",
        "        \n",
        "        # Extract data for each station\n",
        "        for station_id, station_info in stations.items():\n",
        "            # Extract data at station location\n",
        "            station_data = ds.tasmaxAdjust.sel(\n",
        "                lat=station_info['lat'], \n",
        "                lon=station_info['lon'], \n",
        "                method='nearest'\n",
        "            )\n",
        "            \n",
        "            # Get grid coordinates\n",
        "            grid_lat = float(station_data.lat.values)\n",
        "            grid_lon = float(station_data.lon.values)\n",
        "            distance_km = np.sqrt((grid_lat - station_info['lat'])**2 + \n",
        "                                (grid_lon - station_info['lon'])**2) * 111\n",
        "            \n",
        "            # Store extracted data\n",
        "            model_station_data[model_name][station_id] = station_data.values\n",
        "            \n",
        "            # Log extraction info\n",
        "            extraction_log.append({\n",
        "                'Model': model_name,\n",
        "                'Station': station_id,\n",
        "                'Station_Name': station_info['name'],\n",
        "                'Distance_km': round(distance_km, 2),\n",
        "                'Days_Extracted': len(station_data),\n",
        "                'Valid_Days': int(np.sum(~np.isnan(station_data.values)))\n",
        "            })\n",
        "            \n",
        "            print(f\"    ‚úÖ {station_id}: {len(station_data)} days, distance: {distance_km:.2f} km\")\n",
        "        \n",
        "        ds.close()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Part 1 Complete: {len(extraction_log)} extractions\")\n",
        "    return model_station_data, pd.DataFrame(extraction_log)\n",
        "\n",
        "# Execute Part 1\n",
        "extracted_data, extraction_summary = extract_station_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "part1_visualization"
      },
      "outputs": [],
      "source": [
        "# Create Part 1 visualization\n",
        "def create_part1_visualization():\n",
        "    \"\"\"Create visualization for Part 1 results\"\"\"\n",
        "    \n",
        "    print(\"üìä CREATING PART 1 VISUALIZATION\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    # Create temperature summary for visualization\n",
        "    summary_data = []\n",
        "    for model_name in extracted_data.keys():\n",
        "        for station_id in extracted_data[model_name].keys():\n",
        "            temp_data = extracted_data[model_name][station_id]\n",
        "            summary_data.append({\n",
        "                'Model': model_name,\n",
        "                'Station': station_id,\n",
        "                'Station_Name': stations[station_id]['name'],\n",
        "                'Mean_Temp': round(np.nanmean(temp_data), 2),\n",
        "                'Min_Temp': round(np.nanmin(temp_data), 2),\n",
        "                'Max_Temp': round(np.nanmax(temp_data), 2)\n",
        "            })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
        "\n",
        "    # 1. Distance to grid points\n",
        "    ax1 = axes[0]\n",
        "    extraction_summary.boxplot(column='Distance_km', by='Station', ax=ax1)\n",
        "    ax1.set_title('Distance to Nearest Grid Point by Station\\n(Spatial Resolution Check)', \n",
        "                  fontsize=11, fontweight='bold', pad=15)\n",
        "    ax1.set_ylabel('Distance (km)', fontsize=10)\n",
        "    ax1.set_xlabel('Station ID', fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Temperature ranges by model\n",
        "    ax2 = axes[1]\n",
        "    models = list(model_colors.keys())\n",
        "    model_temps = [summary_df[summary_df['Model'] == model]['Mean_Temp'].values for model in models]\n",
        "\n",
        "    bp = ax2.boxplot(model_temps, labels=models, patch_artist=True, widths=0.6)\n",
        "    for patch, model in zip(bp['boxes'], models):\n",
        "        patch.set_facecolor(model_colors[model])\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    ax2.set_title('Mean Temperature by Model\\n1990-2014 Daily Average', \n",
        "                  fontsize=11, fontweight='bold', pad=15)\n",
        "    ax2.set_ylabel('Temperature (¬∞C)', fontsize=10)\n",
        "    ax2.set_xlabel('Climate Model', fontsize=10)\n",
        "    ax2.tick_params(axis='x', rotation=45, labelsize=9)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Temperature by station (with model colors)\n",
        "    ax3 = axes[2]\n",
        "    stations_order = ['AL0019', 'AL0035', 'AL0059']\n",
        "\n",
        "    for model in models:\n",
        "        model_data = summary_df[summary_df['Model'] == model]\n",
        "        temps = []\n",
        "        x_positions = []\n",
        "        \n",
        "        for i, station_id in enumerate(stations_order):\n",
        "            station_temp = model_data[model_data['Station'] == station_id]['Mean_Temp']\n",
        "            if not station_temp.empty:\n",
        "                temps.append(station_temp.values[0])\n",
        "                x_positions.append(i)\n",
        "        \n",
        "        if temps:\n",
        "            ax3.scatter(x_positions, temps, label=model, color=model_colors[model], \n",
        "                       alpha=0.8, s=80, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    ax3.set_title('Mean Temperature by Station\\n1990-2014 Daily Average (All Models)', \n",
        "                  fontsize=11, fontweight='bold', pad=15)\n",
        "    ax3.set_ylabel('Temperature (¬∞C)', fontsize=10)\n",
        "    ax3.set_xlabel('Station Location', fontsize=10)\n",
        "    ax3.set_xticks(range(len(stations_order)))\n",
        "    ax3.set_xticklabels([f\"{sid}\\n({stations[sid]['name']})\" for sid in stations_order], fontsize=8)\n",
        "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    fig.suptitle('Part 1: Climate Model Data Extraction Results\\nPeriod: 1990-2014 (25 years, daily data)', \n",
        "                 fontsize=13, fontweight='bold', y=0.95)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.82, right=0.85, bottom=0.15, wspace=0.35)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Part 1 visualization complete!\")\n",
        "\n",
        "# Create the visualization\n",
        "create_part1_visualization()\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nüéØ PART 1 SUMMARY\")\n",
        "print(\"=\" * 20)\n",
        "print(f\"‚úÖ Models processed: {len(extracted_data)}\")\n",
        "print(f\"‚úÖ Stations extracted: {len(stations)}\")\n",
        "print(f\"‚úÖ Total extractions: {len(extraction_summary)}\")\n",
        "\n",
        "avg_distance = extraction_summary['Distance_km'].mean()\n",
        "avg_completeness = ((extraction_summary['Valid_Days'].sum() / extraction_summary['Days_Extracted'].sum()) * 100)\n",
        "\n",
        "print(f\"\\nüìä Data Quality:\")\n",
        "print(f\"  Average distance to grid: {avg_distance:.2f} km\")\n",
        "print(f\"  Average data completeness: {avg_completeness:.1f}%\")\n",
        "\n",
        "print(f\"\\n‚û°Ô∏è Ready for Part 2: Monthly Climatology Calculation\")\n",
        "print(f\"üîó Data stored in memory for seamless workflow\")"
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "# üìä Part 2: Monthly Climatology Calculation\n",
        "\n",
        "Convert the daily temperature data to monthly climatology to see seasonal patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate_climatology"
      },
      "outputs": [],
      "source": [
        "def calculate_monthly_climatology_from_memory(extracted_data):\n",
        "    \"\"\"Calculate monthly climatology from extracted data in memory\"\"\"\n",
        "    \n",
        "    print(f\"üìä CALCULATING MONTHLY CLIMATOLOGY\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    climatology_data = []\n",
        "    \n",
        "    # Process each model-station combination\n",
        "    for model_name in extracted_data.keys():\n",
        "        for station_id in extracted_data[model_name].keys():\n",
        "            \n",
        "            print(f\"  üîÑ Processing {model_name} - {station_id}\")\n",
        "            \n",
        "            # Create time index and DataFrame\n",
        "            temp_data = extracted_data[model_name][station_id]\n",
        "            time_index = pd.date_range('1990-01-01', periods=len(temp_data), freq='D')\n",
        "            \n",
        "            df = pd.DataFrame({\n",
        "                'Date': time_index,\n",
        "                'Temperature_C': temp_data\n",
        "            })\n",
        "            \n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            \n",
        "            # Calculate monthly climatology\n",
        "            monthly_clim = df.groupby('Month')['Temperature_C'].agg([\n",
        "                ('Mean_Temp', 'mean'),\n",
        "                ('Min_Temp', 'min'),\n",
        "                ('Max_Temp', 'max'),\n",
        "                ('Std_Temp', 'std')\n",
        "            ]).round(2)\n",
        "            \n",
        "            # Add month names and identification\n",
        "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "            monthly_clim['Month_Name'] = month_names\n",
        "            monthly_clim['Model'] = model_name\n",
        "            monthly_clim['Station'] = station_id\n",
        "            monthly_clim['Station_Name'] = stations[station_id]['name']\n",
        "            monthly_clim = monthly_clim.reset_index()\n",
        "            \n",
        "            climatology_data.append(monthly_clim)\n",
        "    \n",
        "    # Combine all climatology data\n",
        "    all_climatology = pd.concat(climatology_data, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Part 2 Complete: Monthly climatology for {len(climatology_data)} model-station combinations\")\n",
        "    return all_climatology\n",
        "\n",
        "# Execute Part 2\n",
        "model_climatology = calculate_monthly_climatology_from_memory(extracted_data)\n",
        "\n",
        "if model_climatology is not None:\n",
        "    print(f\"\\nüìä Climatology Summary:\")\n",
        "    print(f\"  Total records: {len(model_climatology)}\")\n",
        "    print(f\"  Models: {model_climatology['Model'].nunique()}\")\n",
        "    print(f\"  Stations: {model_climatology['Station'].nunique()}\")\n",
        "    print(f\"  Temperature range: {model_climatology['Mean_Temp'].min():.1f}¬∞C to {model_climatology['Mean_Temp'].max():.1f}¬∞C\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "climatology_visualization"
      },
      "outputs": [],
      "source": [
        "def create_climatology_visualization(climatology_df):\n",
        "    \"\"\"Create comprehensive climatology visualization\"\"\"\n",
        "    \n",
        "    print(f\"üìà CREATING CLIMATOLOGY VISUALIZATION\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Create subplots for each station\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    stations_order = ['AL0019', 'AL0035', 'AL0059']\n",
        "    \n",
        "    for i, station_id in enumerate(stations_order):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Plot each model for this station\n",
        "        for model in climatology_df['Model'].unique():\n",
        "            data = climatology_df[(climatology_df['Model'] == model) & \n",
        "                                (climatology_df['Station'] == station_id)]\n",
        "            \n",
        "            if not data.empty:\n",
        "                months = data['Month'].values\n",
        "                temps = data['Mean_Temp'].values\n",
        "                \n",
        "                ax.plot(months, temps, 'o-', color=model_colors[model], \n",
        "                       label=model, linewidth=2, markersize=6, alpha=0.8)\n",
        "        \n",
        "        # Formatting\n",
        "        ax.set_title(f'{station_id}\\n({stations[station_id][\"name\"]})', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel('Month', fontsize=10)\n",
        "        ax.set_ylabel('Temperature (¬∞C)', fontsize=10)\n",
        "        ax.set_xticks(range(1, 13))\n",
        "        ax.set_xticklabels(['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D'])\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add legend only to the last subplot\n",
        "        if i == 2:\n",
        "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "    \n",
        "    # Main title\n",
        "    fig.suptitle('Part 2: Monthly Temperature Climatology by Station\\n1990-2014 Average (All Models)', \n",
        "                 fontsize=14, fontweight='bold', y=0.98)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.85, right=0.85)\n",
        "    \n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Climatology visualization complete!\")\n",
        "\n",
        "# Create the visualization\n",
        "if model_climatology is not None:\n",
        "    create_climatology_visualization(model_climatology)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seasonal_analysis"
      },
      "outputs": [],
      "source": [
        "def calculate_seasonal_summary(climatology_df):\n",
        "    \"\"\"Calculate seasonal temperature summary\"\"\"\n",
        "    \n",
        "    print(f\"üå°Ô∏è CALCULATING SEASONAL SUMMARY\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Define seasons\n",
        "    seasons = {\n",
        "        'Winter': [12, 1, 2],\n",
        "        'Spring': [3, 4, 5],\n",
        "        'Summer': [6, 7, 8],\n",
        "        'Autumn': [9, 10, 11]\n",
        "    }\n",
        "    \n",
        "    seasonal_data = []\n",
        "    \n",
        "    for model in climatology_df['Model'].unique():\n",
        "        for station in climatology_df['Station'].unique():\n",
        "            data = climatology_df[(climatology_df['Model'] == model) & \n",
        "                                (climatology_df['Station'] == station)]\n",
        "            \n",
        "            if not data.empty:\n",
        "                row = {\n",
        "                    'Model': model,\n",
        "                    'Station': station,\n",
        "                    'Station_Name': stations[station]['name']\n",
        "                }\n",
        "                \n",
        "                for season, months in seasons.items():\n",
        "                    season_temps = data[data['Month'].isin(months)]['Mean_Temp']\n",
        "                    row[f'{season}_Temp'] = round(season_temps.mean(), 2)\n",
        "                \n",
        "                # Annual average\n",
        "                row['Annual_Temp'] = round(data['Mean_Temp'].mean(), 2)\n",
        "                \n",
        "                seasonal_data.append(row)\n",
        "    \n",
        "    seasonal_df = pd.DataFrame(seasonal_data)\n",
        "    \n",
        "    # Display summary statistics\n",
        "    print(f\"\\nüå°Ô∏è Seasonal Temperature Ranges:\")\n",
        "    for season in ['Winter', 'Spring', 'Summer', 'Autumn']:\n",
        "        col_name = f'{season}_Temp'\n",
        "        min_temp = seasonal_df[col_name].min()\n",
        "        max_temp = seasonal_df[col_name].max()\n",
        "        print(f\"  {season}: {min_temp:.1f}¬∞C to {max_temp:.1f}¬∞C\")\n",
        "    \n",
        "    return seasonal_df\n",
        "\n",
        "# Calculate seasonal summary\n",
        "if model_climatology is not None:\n",
        "    seasonal_summary = calculate_seasonal_summary(model_climatology)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "part2_data_preview"
      },
      "outputs": [],
      "source": [
        "# Display sample climatology data and analysis\n",
        "if model_climatology is not None:\n",
        "    print(\"üìã PART 2 DATA PREVIEW\")\n",
        "    print(\"=\" * 25)\n",
        "    \n",
        "    # Show first few rows\n",
        "    print(\"\\nüîç Sample Climatology Data (First 10 records):\")\n",
        "    display(model_climatology[['Model', 'Station', 'Month_Name', 'Mean_Temp', 'Min_Temp', 'Max_Temp']].head(10))\n",
        "    \n",
        "    # Show summary by model\n",
        "    print(\"\\nüìä Average Temperature by Model (across all stations and months):\")\n",
        "    model_avg = model_climatology.groupby('Model')['Mean_Temp'].mean().round(2)\n",
        "    for model, temp in model_avg.items():\n",
        "        print(f\"  {model}: {temp:.2f}¬∞C\")\n",
        "    \n",
        "    # Show summary by station\n",
        "    print(\"\\nüìç Average Temperature by Station (across all models and months):\")\n",
        "    station_avg = model_climatology.groupby(['Station', 'Station_Name'])['Mean_Temp'].mean().round(2)\n",
        "    for (station_id, station_name), temp in station_avg.items():\n",
        "        print(f\"  {station_id} ({station_name}): {temp:.2f}¬∞C\")\n",
        "    \n",
        "    # Monthly temperature range\n",
        "    print(f\"\\nüå°Ô∏è Monthly Temperature Patterns:\")\n",
        "    monthly_avg = model_climatology.groupby('Month_Name')['Mean_Temp'].agg(['min', 'max', 'mean']).round(1)\n",
        "    month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "    monthly_avg = monthly_avg.reindex(month_order)\n",
        "    \n",
        "    for month, row in monthly_avg.iterrows():\n",
        "        print(f\"  {month}: {row['min']:.1f}¬∞C to {row['max']:.1f}¬∞C (avg: {row['mean']:.1f}¬∞C)\")\n",
        "else:\n",
        "    print(\"‚ùå No climatology data available for preview\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "part2_summary"
      },
      "outputs": [],
      "source": [
        "# Part 2 Summary\n",
        "if model_climatology is not None:\n",
        "    print(\"\\nüéØ PART 2 SUMMARY\")\n",
        "    print(\"=\" * 20)\n",
        "    print(f\"‚úÖ Monthly climatology calculated for {len(model_climatology['Model'].unique())} models\")\n",
        "    print(f\"‚úÖ {len(model_climatology['Station'].unique())} stations processed\")\n",
        "    print(f\"‚úÖ 12 months √ó {len(model_climatology['Model'].unique())} models √ó {len(model_climatology['Station'].unique())} stations = {len(model_climatology)} records\")\n",
        "    \n",
        "    print(f\"\\nüìä Temperature Ranges:\")\n",
        "    print(f\"  Coldest month average: {model_climatology['Mean_Temp'].min():.1f}¬∞C\")\n",
        "    print(f\"  Warmest month average: {model_climatology['Mean_Temp'].max():.1f}¬∞C\")\n",
        "    print(f\"  Annual temperature range: {model_climatology['Mean_Temp'].max() - model_climatology['Mean_Temp'].min():.1f}¬∞C\")\n",
        "    \n",
        "    print(f\"\\nüéì Key Learning Points:\")\n",
        "    print(f\"  ‚Ä¢ Monthly climatology reveals seasonal temperature patterns\")\n",
        "    print(f\"  ‚Ä¢ All models show similar seasonal cycles (summer peak, winter minimum)\")\n",
        "    print(f\"  ‚Ä¢ Small but consistent differences between models\")\n",
        "    print(f\"  ‚Ä¢ Spatial temperature gradient maintained across seasons\")\n",
        "    print(f\"  ‚Ä¢ Data ready for validation against observations\")\n",
        "    \n",
        "    print(f\"\\n‚û°Ô∏è Ready for Part 3: Station Data Processing\")\n",
        "    print(f\"üîó Model climatology stored in memory for validation\")\n",
        "else:\n",
        "    print(\"‚ùå Part 2 could not be completed. Please check Part 1 results.\")"
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_header"
      },
      "source": [
        "# üå°Ô∏è Part 3: Station Data Processing\n",
        "\n",
        "Process observed temperature data from weather stations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process_station_data"
      },
      "outputs": [],
      "source": [
        "def process_station_data():\n",
        "    \"\"\"Process observed station data for validation\"\"\"\n",
        "    \n",
        "    print(f\"üå°Ô∏è PROCESSING STATION DATA\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    station_climatology = []\n",
        "    target_stations = ['AL0019', 'AL0035', 'AL0059']\n",
        "    \n",
        "    for station_id in target_stations:\n",
        "        station_file = f\"input_data/stations_data/{station_id}.xlsx\"\n",
        "        \n",
        "        if not os.path.exists(station_file):\n",
        "            print(f\"‚ùå Station file not found: {station_id}.xlsx\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"üîÑ Processing station: {station_id}\")\n",
        "        \n",
        "        try:\n",
        "            # Read and process station data\n",
        "            df = pd.read_excel(station_file)\n",
        "            \n",
        "            if 'Date' not in df.columns or 'Corrected Tmax' not in df.columns:\n",
        "                print(f\"‚ùå Missing required columns in {station_id}\")\n",
        "                continue\n",
        "            \n",
        "            # Filter for study period and clean data\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df = df[(df['Date'].dt.year >= 1990) & (df['Date'].dt.year <= 2014)]\n",
        "            df['Temperature'] = pd.to_numeric(df['Corrected Tmax'], errors='coerce')\n",
        "            df = df[(df['Temperature'] >= -20) & (df['Temperature'] <= 60)]\n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            \n",
        "            # Calculate monthly climatology\n",
        "            monthly_clim = df.groupby('Month')['Temperature'].agg([\n",
        "                ('Mean_Temp', 'mean'),\n",
        "                ('Min_Temp', 'min'),\n",
        "                ('Max_Temp', 'max'),\n",
        "                ('Std_Temp', 'std'),\n",
        "                ('Count', 'count')\n",
        "            ]).round(2)\n",
        "            \n",
        "            # Add identification\n",
        "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "            monthly_clim['Month_Name'] = month_names\n",
        "            monthly_clim['Station'] = station_id\n",
        "            monthly_clim['Station_Name'] = stations[station_id]['name']\n",
        "            monthly_clim = monthly_clim.reset_index()\n",
        "            \n",
        "            station_climatology.append(monthly_clim)\n",
        "            \n",
        "            valid_days = df['Temperature'].notna().sum()\n",
        "            total_days = len(df)\n",
        "            completeness = (valid_days / total_days) * 100\n",
        "            \n",
        "            print(f\"  ‚úÖ {station_id}: {completeness:.1f}% data completeness\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error processing {station_id}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if station_climatology:\n",
        "        combined_station_clim = pd.concat(station_climatology, ignore_index=True)\n",
        "        print(f\"\\n‚úÖ Part 3 Complete: Station climatology for {len(station_climatology)} stations\")\n",
        "        return combined_station_clim\n",
        "    else:\n",
        "        print(\"‚ùå No station data processed\")\n",
        "        return None\n",
        "\n",
        "# Execute Part 3\n",
        "station_climatology = process_station_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "station_visualization"
      },
      "outputs": [],
      "source": [
        "# Create station data visualization\n",
        "if station_climatology is not None:\n",
        "    \n",
        "    print(\"üìä CREATING STATION VISUALIZATION\")\n",
        "    \n",
        "    station_colors = {'AL0019': '#1f77b4', 'AL0035': '#ff7f0e', 'AL0059': '#2ca02c'}\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # 1. Monthly climatology\n",
        "    ax1 = axes[0]\n",
        "    for station_id in ['AL0019', 'AL0035', 'AL0059']:\n",
        "        station_clim = station_climatology[station_climatology['Station'] == station_id]\n",
        "        if not station_clim.empty:\n",
        "            months = station_clim['Month'].values\n",
        "            temps = station_clim['Mean_Temp'].values\n",
        "            ax1.plot(months, temps, 'o-', color=station_colors[station_id], \n",
        "                    label=f'{station_id} ({stations[station_id][\"name\"]})', linewidth=2, markersize=6)\n",
        "    \n",
        "    ax1.set_title('Observed Monthly Temperature Climatology\\n1990-2014 Average', fontweight='bold')\n",
        "    ax1.set_xlabel('Month')\n",
        "    ax1.set_ylabel('Temperature (¬∞C)')\n",
        "    ax1.set_xticks(range(1, 13))\n",
        "    ax1.set_xticklabels(['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D'])\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend(fontsize=9)\n",
        "    \n",
        "    # 2. Data completeness\n",
        "    ax2 = axes[1]\n",
        "    stations_processed = []\n",
        "    data_completeness = []\n",
        "    \n",
        "    for station_id in ['AL0019', 'AL0035', 'AL0059']:\n",
        "        if station_id in station_climatology['Station'].values:\n",
        "            stations_processed.append(station_id)\n",
        "            # Estimate completeness from count data\n",
        "            station_data = station_climatology[station_climatology['Station'] == station_id]\n",
        "            avg_count = station_data['Count'].mean()\n",
        "            expected_days_per_month = 25 * 365.25 / 12  # 25 years average\n",
        "            completeness = min(100, (avg_count / expected_days_per_month) * 100)\n",
        "            data_completeness.append(completeness)\n",
        "    \n",
        "    bars = ax2.bar(range(len(stations_processed)), data_completeness, \n",
        "                   color=[station_colors[sid] for sid in stations_processed], alpha=0.7, edgecolor='black')\n",
        "    \n",
        "    ax2.set_title('Station Data Quality\\n1990-2014 Period', fontweight='bold')\n",
        "    ax2.set_ylabel('Data Completeness (%)')\n",
        "    ax2.set_xlabel('Station')\n",
        "    ax2.set_xticks(range(len(stations_processed)))\n",
        "    ax2.set_xticklabels(stations_processed)\n",
        "    ax2.set_ylim(0, 105)\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add percentage labels on bars\n",
        "    for bar, pct in zip(bars, data_completeness):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    fig.suptitle('Part 3: Station Data Processing Results', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ Station visualization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "part3_summary"
      },
      "outputs": [],
      "source": [
        "# Part 3 Summary\n",
        "if station_climatology is not None:\n",
        "    print(\"\\nüéØ PART 3 SUMMARY\")\n",
        "    print(\"=\" * 20)\n",
        "    print(f\"‚úÖ Stations processed: {len(station_climatology['Station'].unique())}\")\n",
        "    print(f\"‚úÖ Total monthly records: {len(station_climatology)}\")\n",
        "    print(f\"‚úÖ Study period: 1990-2014 (25 years)\")\n",
        "    \n",
        "    print(f\"\\nüìä Station Data Quality:\")\n",
        "    for station_id in station_climatology['Station'].unique():\n",
        "        station_data = station_climatology[station_climatology['Station'] == station_id]\n",
        "        avg_temp = station_data['Mean_Temp'].mean()\n",
        "        print(f\"  {station_id}: avg temp {avg_temp:.1f}¬∞C\")\n",
        "    \n",
        "    print(f\"\\n‚û°Ô∏è Ready for Part 4: Model Validation\")\n",
        "    print(f\"üîó Station climatology ready for comparison with models\")\n",
        "else:\n",
        "    print(\"‚ùå Part 3 failed - no station data available\")"
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4_header"
      },
      "source": [
        "# üéì Part 4: Model Validation\n",
        "\n",
        "Validate climate models against observations using 5 statistical metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validation_functions"
      },
      "outputs": [],
      "source": [
        "def calculate_validation_metrics(model_temps, obs_temps):\n",
        "    \"\"\"Calculate the 5 validation metrics\"\"\"\n",
        "    \n",
        "    # Remove NaN values\n",
        "    valid_mask = ~(np.isnan(model_temps) | np.isnan(obs_temps))\n",
        "    model_valid = model_temps[valid_mask]\n",
        "    obs_valid = obs_temps[valid_mask]\n",
        "    \n",
        "    if len(model_valid) < 3:\n",
        "        return None\n",
        "    \n",
        "    metrics = {}\n",
        "    \n",
        "    # 1. Root Mean Square Error (RMSE)\n",
        "    metrics['RMSE'] = np.sqrt(np.mean((model_valid - obs_valid) ** 2))\n",
        "    \n",
        "    # 2. Correlation coefficient (R)\n",
        "    metrics['R'] = np.corrcoef(model_valid, obs_valid)[0, 1]\n",
        "    \n",
        "    # 3. Nash-Sutcliffe Efficiency (NSE)\n",
        "    mean_obs = np.mean(obs_valid)\n",
        "    metrics['NSE'] = 1 - (np.sum((model_valid - obs_valid) ** 2) / \n",
        "                         np.sum((obs_valid - mean_obs) ** 2))\n",
        "    \n",
        "    # 4. Percent Bias (PBIAS)\n",
        "    metrics['PBIAS'] = 100 * (np.mean(model_valid - obs_valid) / np.mean(obs_valid))\n",
        "    \n",
        "    # 5. Mean Absolute Error (MAE)\n",
        "    metrics['MAE'] = np.mean(np.abs(model_valid - obs_valid))\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def perform_validation(model_clim, station_clim):\n",
        "    \"\"\"Perform complete model validation\"\"\"\n",
        "    \n",
        "    print(f\"üéì PERFORMING MODEL VALIDATION\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    if model_clim is None or station_clim is None:\n",
        "        print(\"‚ùå Missing climatology data for validation\")\n",
        "        return None, None\n",
        "    \n",
        "    validation_results = []\n",
        "    models = model_clim['Model'].unique()\n",
        "    station_ids = station_clim['Station'].unique()\n",
        "    \n",
        "    # Process each model-station combination\n",
        "    for model in models:\n",
        "        for station_id in station_ids:\n",
        "            \n",
        "            print(f\"  üîÑ Validating {model} at {station_id}\")\n",
        "            \n",
        "            # Get model data\n",
        "            model_data = model_clim[(model_clim['Model'] == model) & \n",
        "                                  (model_clim['Station'] == station_id)]\n",
        "            \n",
        "            # Get station data\n",
        "            station_data = station_clim[station_clim['Station'] == station_id]\n",
        "            \n",
        "            if model_data.empty or station_data.empty:\n",
        "                print(f\"    ‚ùå No data available\")\n",
        "                continue\n",
        "            \n",
        "            # Extract monthly temperature arrays\n",
        "            model_temps = model_data['Mean_Temp'].values\n",
        "            obs_temps = station_data['Mean_Temp'].values\n",
        "            \n",
        "            # Calculate metrics\n",
        "            metrics = calculate_validation_metrics(model_temps, obs_temps)\n",
        "            \n",
        "            if metrics is None:\n",
        "                print(f\"    ‚ùå Insufficient data for validation\")\n",
        "                continue\n",
        "            \n",
        "            # Store results\n",
        "            result = {\n",
        "                'Model': model,\n",
        "                'Station': station_id,\n",
        "                'Station_Name': stations[station_id]['name'],\n",
        "                **metrics\n",
        "            }\n",
        "            validation_results.append(result)\n",
        "            \n",
        "            print(f\"    ‚úÖ R={metrics['R']:.3f}, RMSE={metrics['RMSE']:.2f}¬∞C, NSE={metrics['NSE']:.3f}\")\n",
        "    \n",
        "    if not validation_results:\n",
        "        print(\"‚ùå No validation results generated\")\n",
        "        return None, None\n",
        "    \n",
        "    results_df = pd.DataFrame(validation_results)\n",
        "    \n",
        "    # Calculate model ranking\n",
        "    model_ranking = results_df.groupby('Model').agg({\n",
        "        'RMSE': 'mean',\n",
        "        'R': 'mean',\n",
        "        'NSE': 'mean', \n",
        "        'PBIAS': lambda x: np.mean(np.abs(x)),\n",
        "        'MAE': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    # Calculate overall rank\n",
        "    model_ranking['RMSE_rank'] = model_ranking['RMSE'].rank(ascending=True)\n",
        "    model_ranking['R_rank'] = model_ranking['R'].rank(ascending=False)\n",
        "    model_ranking['NSE_rank'] = model_ranking['NSE'].rank(ascending=False)\n",
        "    model_ranking['PBIAS_rank'] = model_ranking['PBIAS'].rank(ascending=True)\n",
        "    model_ranking['MAE_rank'] = model_ranking['MAE'].rank(ascending=True)\n",
        "    \n",
        "    rank_cols = ['RMSE_rank', 'R_rank', 'NSE_rank', 'PBIAS_rank', 'MAE_rank']\n",
        "    model_ranking['Average_Rank'] = model_ranking[rank_cols].mean(axis=1)\n",
        "    model_ranking = model_ranking.sort_values('Average_Rank')\n",
        "    \n",
        "    print(f\"\\n‚úÖ Part 4 Complete: {len(results_df)} validations performed\")\n",
        "    return results_df, model_ranking\n",
        "\n",
        "# Execute Part 4\n",
        "validation_results, model_ranking = perform_validation(model_climatology, station_climatology)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validation_visualization"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive validation visualization\n",
        "if validation_results is not None and model_ranking is not None:\n",
        "    \n",
        "    print(\"üìä CREATING VALIDATION VISUALIZATION\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # 1. RMSE Heatmap\n",
        "    ax1 = axes[0, 0]\n",
        "    rmse_pivot = validation_results.pivot(index='Station', columns='Model', values='RMSE')\n",
        "    sns.heatmap(rmse_pivot, annot=True, fmt='.2f', cmap='Reds', ax=ax1, cbar_kws={'label': 'RMSE (¬∞C)'})\n",
        "    ax1.set_title('RMSE\\n(Lower is Better)', fontweight='bold')\n",
        "    \n",
        "    # 2. Correlation Heatmap\n",
        "    ax2 = axes[0, 1]\n",
        "    r_pivot = validation_results.pivot(index='Station', columns='Model', values='R')\n",
        "    sns.heatmap(r_pivot, annot=True, fmt='.3f', cmap='Blues', ax=ax2, cbar_kws={'label': 'Correlation (R)'})\n",
        "    ax2.set_title('Correlation\\n(Higher is Better)', fontweight='bold')\n",
        "    \n",
        "    # 3. NSE Heatmap\n",
        "    ax3 = axes[0, 2]\n",
        "    nse_pivot = validation_results.pivot(index='Station', columns='Model', values='NSE')\n",
        "    sns.heatmap(nse_pivot, annot=True, fmt='.3f', cmap='Greens', ax=ax3, cbar_kws={'label': 'NSE'})\n",
        "    ax3.set_title('Nash-Sutcliffe Efficiency\\n(Higher is Better)', fontweight='bold')\n",
        "    \n",
        "    # 4. PBIAS Heatmap\n",
        "    ax4 = axes[1, 0]\n",
        "    pbias_pivot = validation_results.pivot(index='Station', columns='Model', values='PBIAS')\n",
        "    sns.heatmap(pbias_pivot, annot=True, fmt='.1f', cmap='RdBu_r', center=0, ax=ax4, cbar_kws={'label': 'PBIAS (%)'})\n",
        "    ax4.set_title('Percent Bias\\n(Closer to 0 is Better)', fontweight='bold')\n",
        "    \n",
        "    # 5. Model Performance Box Plot\n",
        "    ax5 = axes[1, 1]\n",
        "    models = list(model_colors.keys())\n",
        "    rmse_data = [validation_results[validation_results['Model'] == model]['RMSE'].values for model in models]\n",
        "    bp = ax5.boxplot(rmse_data, labels=models, patch_artist=True)\n",
        "    for patch, model in zip(bp['boxes'], models):\n",
        "        patch.set_facecolor(model_colors[model])\n",
        "        patch.set_alpha(0.7)\n",
        "    ax5.set_title('RMSE Distribution\\nAcross All Stations', fontweight='bold')\n",
        "    ax5.set_ylabel('RMSE (¬∞C)')\n",
        "    ax5.tick_params(axis='x', rotation=45)\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Model Ranking\n",
        "    ax6 = axes[1, 2]\n",
        "    models_ranked = model_ranking.index\n",
        "    ranks = model_ranking['Average_Rank'].values\n",
        "    colors_list = [model_colors[model] for model in models_ranked]\n",
        "    bars = ax6.barh(range(len(models_ranked)), ranks, color=colors_list, alpha=0.7)\n",
        "    ax6.set_yticks(range(len(models_ranked)))\n",
        "    ax6.set_yticklabels(models_ranked)\n",
        "    ax6.set_xlabel('Average Rank')\n",
        "    ax6.set_title('Model Performance Ranking\\n(Lower is Better)', fontweight='bold')\n",
        "    ax6.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add rank values on bars\n",
        "    for i, (bar, rank) in enumerate(zip(bars, ranks)):\n",
        "        ax6.text(rank + 0.05, bar.get_y() + bar.get_height()/2, \n",
        "                f'{rank:.2f}', ha='left', va='center', fontweight='bold')\n",
        "    \n",
        "    fig.suptitle('Part 4: Climate Model Validation Results\\nAMMAN ZARQA Basin, Jordan (1990-2014)', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.90)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ Validation visualization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_results"
      },
      "outputs": [],
      "source": [
        "# Display final workshop results\n",
        "print(\"üéØ WORKSHOP RESULTS SUMMARY\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if validation_results is not None and model_ranking is not None:\n",
        "    print(f\"‚úÖ Validation completed: {len(validation_results)} model-station combinations\")\n",
        "    print(f\"‚úÖ Models evaluated: {len(validation_results['Model'].unique())}\")\n",
        "    print(f\"‚úÖ Stations evaluated: {len(validation_results['Station'].unique())}\")\n",
        "    \n",
        "    print(f\"\\nüèÜ MODEL RANKING (Best to Worst):\")\n",
        "    for i, (model, row) in enumerate(model_ranking.iterrows(), 1):\n",
        "        rmse = row['RMSE']\n",
        "        r = row['R']\n",
        "        nse = row['NSE']\n",
        "        print(f\"  {i}. {model}: RMSE={rmse:.2f}¬∞C, R={r:.3f}, NSE={nse:.3f}\")\n",
        "    \n",
        "    best_model = model_ranking.index[0]\n",
        "    worst_model = model_ranking.index[-1]\n",
        "    \n",
        "    print(f\"\\nüìä KEY FINDINGS:\")\n",
        "    print(f\"  ü•á Best performing model: {best_model}\")\n",
        "    print(f\"  üìâ Worst performing model: {worst_model}\")\n",
        "    print(f\"  üìè RMSE range: {validation_results['RMSE'].min():.2f} - {validation_results['RMSE'].max():.2f}¬∞C\")\n",
        "    print(f\"  üìà Correlation range: {validation_results['R'].min():.3f} - {validation_results['R'].max():.3f}\")\n",
        "    \n",
        "    print(f\"\\nüìã DETAILED VALIDATION RESULTS:\")\n",
        "    display(validation_results)\n",
        "    \n",
        "    print(f\"\\nüèÜ MODEL RANKING TABLE:\")\n",
        "    display(model_ranking)\n",
        "\n",
        "print(f\"\\nüéâ WORKSHOP COMPLETE!\")\n",
        "print(f\"üéì You have successfully validated climate models for AMMAN ZARQA Basin!\")\n",
        "print(f\"üìö Skills gained: NetCDF processing, climatology calculation, statistical validation\")\n",
        "print(f\"üîó Repository: https://github.com/MoawiahHussien/climate-model-validation-workshop\")"
      ]
    }
  ]
}
