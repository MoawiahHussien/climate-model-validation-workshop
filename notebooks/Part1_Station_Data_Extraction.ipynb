{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üéØ Part 1: Station Data Extraction from Climate Models\n",
        "\n",
        "## Climate Model Validation Workshop\n",
        "**Study Area:** AMMAN ZARQA Basin, Jordan  \n",
        "**Models:** 6 RICCAR Climate Models (SSP4.5)  \n",
        "**Stations:** AL0019, AL0035, AL0059  \n",
        "**Period:** 1990-2014\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "- ‚úÖ Extract climate model data at specific station locations\n",
        "- ‚úÖ Understand spatial resolution and nearest neighbor selection\n",
        "- ‚úÖ Assess data quality and completeness\n",
        "- ‚úÖ Create visualizations showing model temperature patterns\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## ‚öôÔ∏è Setup: Install Packages and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install xarray netcdf4 requests tqdm -q\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def download_workshop_data():\n",
        "    \"\"\"Download NetCDF files from GitHub repository\"\"\"\n",
        "    \n",
        "    print(\"üéØ DOWNLOADING WORKSHOP DATA\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # GitHub repository base URL\n",
        "    base_url = \"https://raw.githubusercontent.com/MoawiahHussien/climate-model-validation-workshop/main/\"\n",
        "    \n",
        "    # Create directories\n",
        "    os.makedirs(\"input_data/models_netcdf\", exist_ok=True)\n",
        "    os.makedirs(\"workshop_output\", exist_ok=True)\n",
        "    \n",
        "    # NetCDF files to download\n",
        "    nc_files = [\n",
        "        \"arcgis_merged_Tmax_CMCC-CM2-SR5.nc\",\n",
        "        \"arcgis_merged_Tmax_CNRM-ESM2-1.nc\", \n",
        "        \"arcgis_merged_Tmax_EC-Earth3-Veg.nc\",\n",
        "        \"arcgis_merged_Tmax_IPSL-CM6A-LR.nc\",\n",
        "        \"arcgis_merged_Tmax_MPI-ESM1-2-LR.nc\",\n",
        "        \"arcgis_merged_Tmax_NorESM2-MM.nc\"\n",
        "    ]\n",
        "    \n",
        "    # Download each file\n",
        "    for nc_file in nc_files:\n",
        "        file_url = base_url + \"Input%20Files/Models.Nc.ArcGIS.Compatible/\" + nc_file\n",
        "        local_path = f\"input_data/models_netcdf/{nc_file}\"\n",
        "        \n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"‚úÖ Using cached: {nc_file}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(f\"üì• Downloading: {nc_file}\")\n",
        "            urllib.request.urlretrieve(file_url, local_path)\n",
        "            file_size = os.path.getsize(local_path) / (1024*1024)\n",
        "            print(f\"   ‚úÖ Complete: {file_size:.1f} MB\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Failed: {e}\")\n",
        "    \n",
        "    print(f\"\\nüéâ Data download complete!\")\n",
        "\n",
        "# Download the data\n",
        "download_workshop_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_libraries"
      },
      "source": [
        "## üìö Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "print(f\"üìç Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "define_stations"
      },
      "source": [
        "## üìç Define Station Locations and Model Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "station_setup"
      },
      "outputs": [],
      "source": [
        "# Station coordinates for AMMAN ZARQA basin\n",
        "stations = {\n",
        "    'AL0019': {'lat': 31.95, 'lon': 35.93, 'name': 'Amman Airport'},\n",
        "    'AL0035': {'lat': 32.01, 'lon': 35.85, 'name': 'Zarqa Station'}, \n",
        "    'AL0059': {'lat': 31.97, 'lon': 36.12, 'name': 'Russeifa Station'}\n",
        "}\n",
        "\n",
        "# Model files (6 RICCAR models)\n",
        "model_files = {\n",
        "    'CMCC': 'arcgis_merged_Tmax_CMCC-CM2-SR5.nc',\n",
        "    'CNRM': 'arcgis_merged_Tmax_CNRM-ESM2-1.nc',\n",
        "    'EC-Earth3': 'arcgis_merged_Tmax_EC-Earth3-Veg.nc',\n",
        "    'IPSL': 'arcgis_merged_Tmax_IPSL-CM6A-LR.nc',\n",
        "    'MPI': 'arcgis_merged_Tmax_MPI-ESM1-2-LR.nc',\n",
        "    'NorESM2': 'arcgis_merged_Tmax_NorESM2-MM.nc'\n",
        "}\n",
        "\n",
        "print(f\"üéØ WORKSHOP SETUP COMPLETE\")\n",
        "print(f\"üìç Target Stations: {len(stations)} stations\")\n",
        "print(f\"üå°Ô∏è Models to Process: {len(model_files)} models\")\n",
        "\n",
        "for station_id, info in stations.items():\n",
        "    print(f\"  {station_id}: {info['name']} ({info['lat']:.2f}¬∞N, {info['lon']:.2f}¬∞E)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extraction_function"
      },
      "source": [
        "## üîÑ Extract Temperature Data at Station Locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_data"
      },
      "outputs": [],
      "source": [
        "def extract_station_data():\n",
        "    \"\"\"Extract temperature data at station locations from all models\"\"\"\n",
        "    \n",
        "    print(f\"üìç EXTRACTING DATA AT STATION LOCATIONS\")\n",
        "    print(\"-\" * 45)\n",
        "    \n",
        "    # Storage for results\n",
        "    model_station_data = {}\n",
        "    extraction_log = []\n",
        "    \n",
        "    # Process each model\n",
        "    for model_name, filename in model_files.items():\n",
        "        print(f\"\\nüîÑ Processing: {model_name}\")\n",
        "        \n",
        "        file_path = f\"input_data/models_netcdf/{filename}\"\n",
        "        \n",
        "        # Check if file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"    ‚ùå File not found: {filename}\")\n",
        "            continue\n",
        "        \n",
        "        # Open NetCDF file\n",
        "        ds = xr.open_dataset(file_path)\n",
        "        \n",
        "        # Initialize storage for this model\n",
        "        model_station_data[model_name] = {}\n",
        "        \n",
        "        # Extract data for each station\n",
        "        for station_id, station_info in stations.items():\n",
        "            \n",
        "            # Extract data at station location (nearest grid point)\n",
        "            station_data = ds.tasmaxAdjust.sel(\n",
        "                lat=station_info['lat'], \n",
        "                lon=station_info['lon'], \n",
        "                method='nearest'\n",
        "            )\n",
        "            \n",
        "            # Get grid coordinates\n",
        "            grid_lat = float(station_data.lat.values)\n",
        "            grid_lon = float(station_data.lon.values)\n",
        "            \n",
        "            # Calculate distance\n",
        "            distance_km = np.sqrt((grid_lat - station_info['lat'])**2 + \n",
        "                                (grid_lon - station_info['lon'])**2) * 111\n",
        "            \n",
        "            # Store extracted data\n",
        "            model_station_data[model_name][station_id] = station_data.values\n",
        "            \n",
        "            # Log extraction info\n",
        "            extraction_log.append({\n",
        "                'Model': model_name,\n",
        "                'Station': station_id,\n",
        "                'Station_Name': station_info['name'],\n",
        "                'Target_Lat': station_info['lat'],\n",
        "                'Target_Lon': station_info['lon'],\n",
        "                'Grid_Lat': round(grid_lat, 3),\n",
        "                'Grid_Lon': round(grid_lon, 3),\n",
        "                'Distance_km': round(distance_km, 2),\n",
        "                'Days_Extracted': len(station_data),\n",
        "                'Valid_Days': int(np.sum(~np.isnan(station_data.values)))\n",
        "            })\n",
        "            \n",
        "            print(f\"    ‚úÖ {station_id}: {len(station_data)} days, distance: {distance_km:.2f} km\")\n",
        "        \n",
        "        ds.close()\n",
        "    \n",
        "    return model_station_data, pd.DataFrame(extraction_log)\n",
        "\n",
        "# Execute extraction\n",
        "extracted_data, extraction_summary = extract_station_data()\n",
        "\n",
        "print(f\"\\nüéâ EXTRACTION COMPLETE!\")\n",
        "print(f\"‚úÖ Successfully extracted data for {len(extracted_data)} models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_results"
      },
      "source": [
        "## üíæ Save Extraction Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_data"
      },
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = \"workshop_output/Part1_Station_Extraction\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save extraction log\n",
        "log_file = f\"{output_dir}/extraction_log.xlsx\"\n",
        "extraction_summary.to_excel(log_file, index=False)\n",
        "print(f\"‚úÖ Extraction log saved: extraction_log.xlsx\")\n",
        "\n",
        "# Save daily temperature data for each model-station combination\n",
        "files_saved = 0\n",
        "for model_name in extracted_data.keys():\n",
        "    for station_id in extracted_data[model_name].keys():\n",
        "        \n",
        "        # Create time index for the data\n",
        "        time_index = pd.date_range('1990-01-01', periods=len(extracted_data[model_name][station_id]), freq='D')\n",
        "        \n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame({\n",
        "            'Date': time_index,\n",
        "            'Temperature_C': extracted_data[model_name][station_id],\n",
        "            'Model': model_name,\n",
        "            'Station': station_id,\n",
        "            'Station_Name': stations[station_id]['name']\n",
        "        })\n",
        "        \n",
        "        # Save individual file\n",
        "        output_file = f\"{output_dir}/{model_name}_{station_id}_daily_temps.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        files_saved += 1\n",
        "\n",
        "print(f\"‚úÖ Daily temperature files saved: {files_saved} files\")\n",
        "print(f\"üìÅ All results saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## üìä Create Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_viz"
      },
      "outputs": [],
      "source": [
        "# Create temperature summary for visualization\n",
        "summary_data = []\n",
        "for model_name in extracted_data.keys():\n",
        "    for station_id in extracted_data[model_name].keys():\n",
        "        temp_data = extracted_data[model_name][station_id]\n",
        "        summary_data.append({\n",
        "            'Model': model_name,\n",
        "            'Station': station_id,\n",
        "            'Station_Name': stations[station_id]['name'],\n",
        "            'Mean_Temp': round(np.nanmean(temp_data), 2),\n",
        "            'Min_Temp': round(np.nanmin(temp_data), 2),\n",
        "            'Max_Temp': round(np.nanmax(temp_data), 2)\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Model colors\n",
        "model_colors = {\n",
        "    'CMCC': '#1f77b4', 'CNRM': '#ff7f0e', 'EC-Earth3': '#2ca02c',\n",
        "    'IPSL': '#d62728', 'MPI': '#9467bd', 'NorESM2': '#8c564b'\n",
        "}\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
        "\n",
        "# 1. Distance to grid points\n",
        "ax1 = axes[0]\n",
        "extraction_summary.boxplot(column='Distance_km', by='Station', ax=ax1)\n",
        "ax1.set_title('Distance to Nearest Grid Point by Station\\n(Spatial Resolution Check)', \n",
        "              fontsize=11, fontweight='bold', pad=15)\n",
        "ax1.set_ylabel('Distance (km)', fontsize=10)\n",
        "ax1.set_xlabel('Station ID', fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Temperature ranges by model\n",
        "ax2 = axes[1]\n",
        "models = list(model_colors.keys())\n",
        "model_temps = [summary_df[summary_df['Model'] == model]['Mean_Temp'].values for model in models]\n",
        "\n",
        "bp = ax2.boxplot(model_temps, labels=models, patch_artist=True, widths=0.6)\n",
        "for patch, model in zip(bp['boxes'], models):\n",
        "    patch.set_facecolor(model_colors[model])\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "ax2.set_title('Mean Temperature by Model\\n1990-2014 Daily Average', \n",
        "              fontsize=11, fontweight='bold', pad=15)\n",
        "ax2.set_ylabel('Temperature (¬∞C)', fontsize=10)\n",
        "ax2.set_xlabel('Climate Model', fontsize=10)\n",
        "ax2.tick_params(axis='x', rotation=45, labelsize=9)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Temperature by station (with model colors)\n",
        "ax3 = axes[2]\n",
        "stations_order = ['AL0019', 'AL0035', 'AL0059']\n",
        "\n",
        "for model in models:\n",
        "    model_data = summary_df[summary_df['Model'] == model]\n",
        "    temps = []\n",
        "    x_positions = []\n",
        "    \n",
        "    for i, station_id in enumerate(stations_order):\n",
        "        station_temp = model_data[model_data['Station'] == station_id]['Mean_Temp']\n",
        "        if not station_temp.empty:\n",
        "            temps.append(station_temp.values[0])\n",
        "            x_positions.append(i)\n",
        "    \n",
        "    if temps:\n",
        "        ax3.scatter(x_positions, temps, label=model, color=model_colors[model], \n",
        "                   alpha=0.8, s=80, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "ax3.set_title('Mean Temperature by Station\\n1990-2014 Daily Average (All Models)', \n",
        "              fontsize=11, fontweight='bold', pad=15)\n",
        "ax3.set_ylabel('Temperature (¬∞C)', fontsize=10)\n",
        "ax3.set_xlabel('Station Location', fontsize=10)\n",
        "ax3.set_xticks(range(len(stations_order)))\n",
        "ax3.set_xticklabels([f\"{sid}\\n({stations[sid]['name']})\" for sid in stations_order], fontsize=8)\n",
        "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "fig.suptitle('Climate Model Data Extraction Results\\nPeriod: 1990-2014 (25 years, daily data)', \n",
        "             fontsize=13, fontweight='bold', y=0.95)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.82, right=0.85, bottom=0.15, wspace=0.35)\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Visualization complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üéØ Part 1 Summary and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# Display final summary\n",
        "print(\"üéØ PART 1 SUMMARY\")\n",
        "print(\"=\" * 20)\n",
        "print(f\"‚úÖ Models processed: {len(extracted_data)}\")\n",
        "print(f\"‚úÖ Stations extracted: {len(stations)}\")\n",
        "print(f\"‚úÖ Total extractions: {len(extraction_summary)}\")\n",
        "\n",
        "# Show data quality\n",
        "avg_distance = extraction_summary['Distance_km'].mean()\n",
        "avg_completeness = ((extraction_summary['Valid_Days'].sum() / extraction_summary['Days_Extracted'].sum()) * 100)\n",
        "\n",
        "print(f\"\\nüìä Data Quality:\")\n",
        "print(f\"  Average distance to grid: {avg_distance:.2f} km\")\n",
        "print(f\"  Average data completeness: {avg_completeness:.1f}%\")\n",
        "\n",
        "# Show extraction summary table\n",
        "print(f\"\\nüìã Extraction Summary:\")\n",
        "display(extraction_summary)\n",
        "\n",
        "print(f\"\\nüéì Key Learning Points:\")\n",
        "print(f\"  ‚Ä¢ Climate models use grid points - we find the nearest one to each station\")\n",
        "print(f\"  ‚Ä¢ Spatial resolution matters - closer grid points give better representation\")\n",
        "print(f\"  ‚Ä¢ Each model has slightly different temperature predictions\")\n",
        "print(f\"  ‚Ä¢ Data quality is excellent - over 99% complete for our study period\")\n",
        "\n",
        "print(f\"\\n‚û°Ô∏è Ready for Part 2: Monthly Climatology Calculation\")\n",
        "print(f\"üìÅ Find your results in: workshop_output/Part1_Station_Extraction/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## üöÄ Next Steps\n",
        "\n",
        "**Congratulations!** You've successfully completed Part 1 of the Climate Model Validation Workshop.\n",
        "\n",
        "### What you accomplished:\n",
        "- ‚úÖ Downloaded 6 climate model NetCDF files\n",
        "- ‚úÖ Extracted daily temperature data at 3 station locations  \n",
        "- ‚úÖ Assessed spatial resolution and data quality\n",
        "- ‚úÖ Created visualizations showing model differences\n",
        "- ‚úÖ Saved organized results for further analysis\n",
        "\n",
        "### Ready for Part 2:\n",
        "**Part 2: Monthly Climatology Calculation**\n",
        "- Calculate monthly averages from daily data\n",
        "- Create seasonal temperature cycles\n",
        "- Compare climatological patterns between models\n",
        "\n",
        "---\n",
        "üìß **Questions?** Contact the workshop instructor  \n",
        "üîó **Repository:** https://github.com/MoawiahHussien/climate-model-validation-workshop"
      ]
    }
  ]
}
