{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "workshop_header"
      },
      "source": [
        "# üå°Ô∏è Climate Model Validation Workshop\n",
        "\n",
        "## Complete End-to-End Analysis\n",
        "**Study Area:** AMMAN ZARQA Basin, Jordan  \n",
        "**Models:** 6 RICCAR Climate Models (SSP4.5)  \n",
        "**Stations:** AL0019, AL0035, AL0059  \n",
        "**Period:** 1990-2014\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Workshop Overview\n",
        "\n",
        "### **Part 1: Station Data Extraction** üìç\n",
        "- Download climate model NetCDF files\n",
        "- Extract temperature data at station locations\n",
        "\n",
        "### **Part 2: Monthly Climatology** üìä\n",
        "- Convert daily data to monthly climatology\n",
        "- Calculate seasonal temperature patterns\n",
        "\n",
        "### **Part 3: Station Data Processing** üå°Ô∏è\n",
        "- Load observed temperature data\n",
        "- Process station climatology\n",
        "\n",
        "### **Part 4: Model Validation** üéì\n",
        "- Calculate 5 validation metrics (RMSE, R, NSE, PBIAS, MAE)\n",
        "- Compare models against observations\n",
        "- Rank model performance\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Learning Objectives\n",
        "- ‚úÖ Extract and process climate data using Python\n",
        "- ‚úÖ Calculate validation metrics and interpret results\n",
        "- ‚úÖ Create publication-quality visualizations\n",
        "- ‚úÖ Make informed decisions about model selection\n",
        "\n",
        "**Estimated Time: 45-60 minutes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "# ‚öôÔ∏è Setup: Install Packages and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "!pip install xarray netcdf4 requests tqdm seaborn -q\n",
        "print(\"‚úÖ Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [],
      "source": [
        "def download_workshop_data():\n",
        "    print(\"üéØ DOWNLOADING WORKSHOP DATA\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    base_url = \"https://raw.githubusercontent.com/MoawiahHussien/climate-model-validation-workshop/main/\"\n",
        "    \n",
        "    os.makedirs(\"input_data/models_netcdf\", exist_ok=True)\n",
        "    os.makedirs(\"input_data/stations_data\", exist_ok=True)\n",
        "    \n",
        "    nc_files = [\n",
        "        \"arcgis_merged_Tmax_CMCC-CM2-SR5.nc\",\n",
        "        \"arcgis_merged_Tmax_CNRM-ESM2-1.nc\", \n",
        "        \"arcgis_merged_Tmax_EC-Earth3-Veg.nc\",\n",
        "        \"arcgis_merged_Tmax_IPSL-CM6A-LR.nc\",\n",
        "        \"arcgis_merged_Tmax_MPI-ESM1-2-LR.nc\",\n",
        "        \"arcgis_merged_Tmax_NorESM2-MM.nc\"\n",
        "    ]\n",
        "    \n",
        "    station_files = [\"AL0019.xlsx\", \"AL0035.xlsx\", \"AL0059.xlsx\"]\n",
        "    \n",
        "    print(\"üì• Downloading climate model files...\")\n",
        "    for nc_file in nc_files:\n",
        "        file_url = base_url + \"Input%20Files/Models.Nc.ArcGIS.Compatible/\" + nc_file\n",
        "        local_path = f\"input_data/models_netcdf/{nc_file}\"\n",
        "        \n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"‚úÖ Using cached: {nc_file}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(f\"üì• Downloading: {nc_file}\")\n",
        "            urllib.request.urlretrieve(file_url, local_path)\n",
        "            file_size = os.path.getsize(local_path) / (1024*1024)\n",
        "            print(f\"   ‚úÖ Complete: {file_size:.1f} MB\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Failed: {e}\")\n",
        "    \n",
        "    print(\"\\nüì• Downloading station files...\")\n",
        "    for station_file in station_files:\n",
        "        file_url = base_url + \"Input%20Files/Stations.Daily/\" + station_file\n",
        "        local_path = f\"input_data/stations_data/{station_file}\"\n",
        "        \n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"‚úÖ Using cached: {station_file}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(f\"üì• Downloading: {station_file}\")\n",
        "            urllib.request.urlretrieve(file_url, local_path)\n",
        "            file_size = os.path.getsize(local_path) / (1024*1024)\n",
        "            print(f\"   ‚úÖ Complete: {file_size:.1f} MB\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Failed: {e}\")\n",
        "    \n",
        "    print(f\"\\nüéâ Data download complete!\")\n",
        "\n",
        "download_workshop_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_constants"
      },
      "outputs": [],
      "source": [
        "stations = {\n",
        "    'AL0019': {'lat': 31.95, 'lon': 35.93, 'name': 'Amman Airport'},\n",
        "    'AL0035': {'lat': 32.01, 'lon': 35.85, 'name': 'Zarqa Station'}, \n",
        "    'AL0059': {'lat': 31.97, 'lon': 36.12, 'name': 'Russeifa Station'}\n",
        "}\n",
        "\n",
        "model_files = {\n",
        "    'CMCC': 'arcgis_merged_Tmax_CMCC-CM2-SR5.nc',\n",
        "    'CNRM': 'arcgis_merged_Tmax_CNRM-ESM2-1.nc',\n",
        "    'EC-Earth3': 'arcgis_merged_Tmax_EC-Earth3-Veg.nc',\n",
        "    'IPSL': 'arcgis_merged_Tmax_IPSL-CM6A-LR.nc',\n",
        "    'MPI': 'arcgis_merged_Tmax_MPI-ESM1-2-LR.nc',\n",
        "    'NorESM2': 'arcgis_merged_Tmax_NorESM2-MM.nc'\n",
        "}\n",
        "\n",
        "model_colors = {\n",
        "    'CMCC': '#1f77b4', 'CNRM': '#ff7f0e', 'EC-Earth3': '#2ca02c',\n",
        "    'IPSL': '#d62728', 'MPI': '#9467bd', 'NorESM2': '#8c564b'\n",
        "}\n",
        "\n",
        "print(f\"üéØ WORKSHOP CONFIGURATION\")\n",
        "print(f\"üìç Target Stations: {len(stations)}\")\n",
        "print(f\"üå°Ô∏è Climate Models: {len(model_files)}\")\n",
        "for station_id, info in stations.items():\n",
        "    print(f\"  {station_id}: {info['name']} ({info['lat']:.2f}¬∞N, {info['lon']:.2f}¬∞E)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_header"
      },
      "source": [
        "# üìç Part 1: Station Data Extraction from Climate Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_station_data"
      },
      "outputs": [],
      "source": [
        "def extract_station_data():\n",
        "    print(f\"üìç EXTRACTING DATA AT STATION LOCATIONS\")\n",
        "    print(\"-\" * 45)\n",
        "    \n",
        "    model_station_data = {}\n",
        "    extraction_log = []\n",
        "    \n",
        "    for model_name, filename in model_files.items():\n",
        "        print(f\"\\nüîÑ Processing: {model_name}\")\n",
        "        \n",
        "        file_path = f\"input_data/models_netcdf/{filename}\"\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"    ‚ùå File not found: {filename}\")\n",
        "            continue\n",
        "        \n",
        "        ds = xr.open_dataset(file_path)\n",
        "        model_station_data[model_name] = {}\n",
        "        \n",
        "        for station_id, station_info in stations.items():\n",
        "            station_data = ds.tasmaxAdjust.sel(\n",
        "                lat=station_info['lat'], \n",
        "                lon=station_info['lon'], \n",
        "                method='nearest'\n",
        "            )\n",
        "            \n",
        "            grid_lat = float(station_data.lat.values)\n",
        "            grid_lon = float(station_data.lon.values)\n",
        "            distance_km = np.sqrt((grid_lat - station_info['lat'])**2 + \n",
        "                                (grid_lon - station_info['lon'])**2) * 111\n",
        "            \n",
        "            model_station_data[model_name][station_id] = station_data.values\n",
        "            \n",
        "            extraction_log.append({\n",
        "                'Model': model_name,\n",
        "                'Station': station_id,\n",
        "                'Station_Name': station_info['name'],\n",
        "                'Distance_km': round(distance_km, 2),\n",
        "                'Days_Extracted': len(station_data),\n",
        "                'Valid_Days': int(np.sum(~np.isnan(station_data.values)))\n",
        "            })\n",
        "            \n",
        "            print(f\"    ‚úÖ {station_id}: {len(station_data)} days, distance: {distance_km:.2f} km\")\n",
        "        \n",
        "        ds.close()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Part 1 Complete: {len(extraction_log)} extractions\")\n",
        "    return model_station_data, pd.DataFrame(extraction_log)\n",
        "\n",
        "extracted_data, extraction_summary = extract_station_data()\n",
        "print(f\"\\n‚û°Ô∏è Ready for Part 2: Monthly Climatology Calculation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "# üìä Part 2: Monthly Climatology Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate_climatology"
      },
      "outputs": [],
      "source": [
        "def calculate_monthly_climatology_from_memory(extracted_data):\n",
        "    print(f\"üìä CALCULATING MONTHLY CLIMATOLOGY\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    climatology_data = []\n",
        "    \n",
        "    for model_name in extracted_data.keys():\n",
        "        for station_id in extracted_data[model_name].keys():\n",
        "            \n",
        "            print(f\"  üîÑ Processing {model_name} - {station_id}\")\n",
        "            \n",
        "            temp_data = extracted_data[model_name][station_id]\n",
        "            time_index = pd.date_range('1990-01-01', periods=len(temp_data), freq='D')\n",
        "            \n",
        "            df = pd.DataFrame({\n",
        "                'Date': time_index,\n",
        "                'Temperature_C': temp_data\n",
        "            })\n",
        "            \n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            \n",
        "            monthly_clim = df.groupby('Month')['Temperature_C'].agg([\n",
        "                ('Mean_Temp', 'mean'),\n",
        "                ('Min_Temp', 'min'),\n",
        "                ('Max_Temp', 'max'),\n",
        "                ('Std_Temp', 'std')\n",
        "            ]).round(2)\n",
        "            \n",
        "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "            monthly_clim['Month_Name'] = month_names\n",
        "            monthly_clim['Model'] = model_name\n",
        "            monthly_clim['Station'] = station_id\n",
        "            monthly_clim['Station_Name'] = stations[station_id]['name']\n",
        "            monthly_clim = monthly_clim.reset_index()\n",
        "            \n",
        "            climatology_data.append(monthly_clim)\n",
        "    \n",
        "    all_climatology = pd.concat(climatology_data, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Part 2 Complete: Monthly climatology for {len(climatology_data)} model-station combinations\")\n",
        "    return all_climatology\n",
        "\n",
        "model_climatology = calculate_monthly_climatology_from_memory(extracted_data)\n",
        "print(f\"\\n‚û°Ô∏è Ready for Part 3: Station Data Processing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_header"
      },
      "source": [
        "# üå°Ô∏è Part 3: Station Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process_station_data"
      },
      "outputs": [],
      "source": [
        "def process_station_data():\n",
        "    print(f\"üå°Ô∏è PROCESSING STATION DATA\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    station_climatology = []\n",
        "    target_stations = ['AL0019', 'AL0035', 'AL0059']\n",
        "    \n",
        "    for station_id in target_stations:\n",
        "        station_file = f\"input_data/stations_data/{station_id}.xlsx\"\n",
        "        \n",
        "        if not os.path.exists(station_file):\n",
        "            print(f\"‚ùå Station file not found: {station_id}.xlsx\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"üîÑ Processing station: {station_id}\")\n",
        "        \n",
        "        try:\n",
        "            df = pd.read_excel(station_file)\n",
        "            \n",
        "            if 'Date' not in df.columns or 'Corrected Tmax' not in df.columns:\n",
        "                print(f\"‚ùå Missing required columns in {station_id}\")\n",
        "                continue\n",
        "            \n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df = df[(df['Date'].dt.year >= 1990) & (df['Date'].dt.year <= 2014)]\n",
        "            df['Temperature'] = pd.to_numeric(df['Corrected Tmax'], errors='coerce')\n",
        "            df = df[(df['Temperature'] >= -20) & (df['Temperature'] <= 60)]\n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            \n",
        "            monthly_clim = df.groupby('Month')['Temperature'].agg([\n",
        "                ('Mean_Temp', 'mean'),\n",
        "                ('Min_Temp', 'min'),\n",
        "                ('Max_Temp', 'max'),\n",
        "                ('Std_Temp', 'std'),\n",
        "                ('Count', 'count')\n",
        "            ]).round(2)\n",
        "            \n",
        "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "            monthly_clim['Month_Name'] = month_names\n",
        "            monthly_clim['Station'] = station_id\n",
        "            monthly_clim['Station_Name'] = stations[station_id]['name']\n",
        "            monthly_clim = monthly_clim.reset_index()\n",
        "            \n",
        "            station_climatology.append(monthly_clim)\n",
        "            \n",
        "            valid_days = df['Temperature'].notna().sum()\n",
        "            total_days = len(df)\n",
        "            completeness = (valid_days / total_days) * 100\n",
        "            \n",
        "            print(f\"  ‚úÖ {station_id}: {completeness:.1f}% data completeness\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error processing {station_id}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if station_climatology:\n",
        "        combined_station_clim = pd.concat(station_climatology, ignore_index=True)\n",
        "        print(f\"\\n‚úÖ Part 3 Complete: Station climatology for {len(station_climatology)} stations\")\n",
        "        return combined_station_clim\n",
        "    else:\n",
        "        print(\"‚ùå No station data processed\")\n",
        "        return None\n",
        "\n",
        "station_climatology = process_station_data()\n",
        "print(f\"\\n‚û°Ô∏è Ready for Part 4: Model Validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4_header"
      },
      "source": [
        "# üéì Part 4: Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validation_functions"
      },
      "outputs": [],
      "source": [
        "def calculate_validation_metrics(model_temps, obs_temps):\n",
        "    valid_mask = ~(np.isnan(model_temps) | np.isnan(obs_temps))\n",
        "    model_valid = model_temps[valid_mask]\n",
        "    obs_valid = obs_temps[valid_mask]\n",
        "    \n",
        "    if len(model_valid) < 3:\n",
        "        return None\n",
        "    \n",
        "    metrics = {}\n",
        "    \n",
        "    # 1. Root Mean Square Error (RMSE)\n",
        "    metrics['RMSE'] = np.sqrt(np.mean((model_valid - obs_valid) ** 2))\n",
        "    \n",
        "    # 2. Correlation coefficient (R)\n",
        "    metrics['R'] = np.corrcoef(model_valid, obs_valid)[0, 1]\n",
        "    \n",
        "    # 3. Nash-Sutcliffe Efficiency (NSE)\n",
        "    mean_obs = np.mean(obs_valid)\n",
        "    metrics['NSE'] = 1 - (np.sum((model_valid - obs_valid) ** 2) / \n",
        "                         np.sum((obs_valid - mean_obs) ** 2))\n",
        "    \n",
        "    # 4. Percent Bias (PBIAS)\n",
        "    metrics['PBIAS'] = 100 * (np.mean(model_valid - obs_valid) / np.mean(obs_valid))\n",
        "    \n",
        "    # 5. Mean Absolute Error (MAE)\n",
        "    metrics['MAE'] = np.mean(np.abs(model_valid - obs_valid))\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def perform_validation(model_clim, station_clim):\n",
        "    print(f\"üéì PERFORMING MODEL VALIDATION\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    if model_clim is None or station_clim is None:\n",
        "        print(\"‚ùå Missing climatology data for validation\")\n",
        "        return None, None\n",
        "    \n",
        "    validation_results = []\n",
        "    models = model_clim['Model'].unique()\n",
        "    station_ids = station_clim['Station'].unique()\n",
        "    \n",
        "    for model in models:\n",
        "        for station_id in station_ids:\n",
        "            \n",
        "            print(f\"  üîÑ Validating {model} at {station_id}\")\n",
        "            \n",
        "            model_data = model_clim[(model_clim['Model'] == model) & \n",
        "                                  (model_clim['Station'] == station_id)]\n",
        "            \n",
        "            station_data = station_clim[station_clim['Station'] == station_id]\n",
        "            \n",
        "            if model_data.empty or station_data.empty:\n",
        "                print(f\"    ‚ùå No data available\")\n",
        "                continue\n",
        "            \n",
        "            model_temps = model_data['Mean_Temp'].values\n",
        "            obs_temps = station_data['Mean_Temp'].values\n",
        "            \n",
        "            metrics = calculate_validation_metrics(model_temps, obs_temps)\n",
        "            \n",
        "            if metrics is None:\n",
        "                print(f\"    ‚ùå Insufficient data for validation\")\n",
        "                continue\n",
        "            \n",
        "            result = {\n",
        "                'Model': model,\n",
        "                'Station': station_id,\n",
        "                'Station_Name': stations[station_id]['name'],\n",
        "                **metrics\n",
        "            }\n",
        "            validation_results.append(result)\n",
        "            \n",
        "            print(f\"    ‚úÖ R={metrics['R']:.3f}, RMSE={metrics['RMSE']:.2f}¬∞C, NSE={metrics['NSE']:.3f}\")\n",
        "    \n",
        "    if not validation_results:\n",
        "        print(\"‚ùå No validation results generated\")\n",
        "        return None, None\n",
        "    \n",
        "    results_df = pd.DataFrame(validation_results)\n",
        "    \n",
        "    # Calculate model ranking\n",
        "    model_ranking = results_df.groupby('Model').agg({\n",
        "        'RMSE': 'mean',\n",
        "        'R': 'mean',\n",
        "        'NSE': 'mean', \n",
        "        'PBIAS': lambda x: np.mean(np.abs(x)),\n",
        "        'MAE': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    model_ranking['RMSE_rank'] = model_ranking['RMSE'].rank(ascending=True)\n",
        "    model_ranking['R_rank'] = model_ranking['R'].rank(ascending=False)\n",
        "    model_ranking['NSE_rank'] = model_ranking['NSE'].rank(ascending=False)\n",
        "    model_ranking['PBIAS_rank'] = model_ranking['PBIAS'].rank(ascending=True)\n",
        "    model_ranking['MAE_rank'] = model_ranking['MAE'].rank(ascending=True)\n",
        "    \n",
        "    rank_cols = ['RMSE_rank', 'R_rank', 'NSE_rank', 'PBIAS_rank', 'MAE_rank']\n",
        "    model_ranking['Average_Rank'] = model_ranking[rank_cols].mean(axis=1)\n",
        "    model_ranking = model_ranking.sort_values('Average_Rank')\n",
        "    \n",
        "    print(f\"\\n‚úÖ Part 4 Complete: {len(results_df)} validations performed\")\n",
        "    return results_df, model_ranking\n",
        "\n",
        "validation_results, model_ranking = perform_validation(model_climatology, station_climatology)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_results"
      },
      "source": [
        "# üéØ Workshop Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "display_final_results"
      },
      "outputs": [],
      "source": [
        "print(\"üéØ WORKSHOP RESULTS SUMMARY\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if validation_results is not None and model_ranking is not None:\n",
        "    print(f\"‚úÖ Validation completed: {len(validation_results)} model-station combinations\")\n",
        "    print(f\"‚úÖ Models evaluated: {len(validation_results['Model'].unique())}\")\n",
        "    print(
